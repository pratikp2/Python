{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain\n",
      "america\n",
      "ate\n",
      "100\n",
      "$\n",
      "of\n",
      "samosa\n",
      ".\n",
      "Then\n",
      "he\n",
      "said\n",
      "I\n",
      "can\n",
      "do\n",
      "this\n",
      "all\n",
      "day\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain   |         |    \n",
      "america   |         |    \n",
      "ate   |         |    \n",
      "100   |         |    \n",
      "$   |         |    \n",
      "of   |         |    \n",
      "samosa   |         |    \n",
      ".   |         |    \n",
      "Then   |         |    \n",
      "he   |         |    \n",
      "said   |         |    \n",
      "I   |         |    \n",
      "can   |         |    \n",
      "do   |         |    \n",
      "this   |         |    \n",
      "all   |         |    \n",
      "day   |         |    \n",
      ".   |         |    \n"
     ]
    }
   ],
   "source": [
    "# Load a default pipe line which is empty and check the output\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \"  |   \", token.pos_, \"    |   \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain   |    PROPN     |    Captain\n",
      "america   |    PROPN     |    america\n",
      "ate   |    VERB     |    eat\n",
      "100   |    NUM     |    100\n",
      "$   |    NUM     |    $\n",
      "of   |    ADP     |    of\n",
      "samosa   |    PROPN     |    samosa\n",
      ".   |    PUNCT     |    .\n",
      "Then   |    ADV     |    then\n",
      "he   |    PRON     |    he\n",
      "said   |    VERB     |    say\n",
      "I   |    PRON     |    I\n",
      "can   |    AUX     |    can\n",
      "do   |    VERB     |    do\n",
      "this   |    PRON     |    this\n",
      "all   |    DET     |    all\n",
      "day   |    NOUN     |    day\n",
      ".   |    PUNCT     |    .\n"
     ]
    }
   ],
   "source": [
    "# load a pre-trained pipeline and check output for same\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \"  |   \", token.pos_, \"    |   \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'] \n",
      "\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x0000024ACF42BEF0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x0000024ACF42A870>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x0000024AD05CC430>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x0000024ACE8DC050>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x0000024AD0BC5C50>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x0000024AD05CC4A0>)] \n",
      "\n",
      "{'tok2vec': 'tok2vec', 'tagger': 'tagger', 'parser': 'parser', 'senter': 'senter', 'attribute_ruler': 'attribute_ruler', 'lemmatizer': 'lemmatizer', 'ner': 'ner'} \n",
      "\n",
      "{'tok2vec': [], 'tagger': ['$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '_SP', '``'], 'parser': ['ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp'], 'attribute_ruler': [], 'lemmatizer': [], 'ner': ['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']} \n",
      "\n",
      "<bound method Language.pipe of <spacy.lang.en.English object at 0x0000024ACE810850>> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(nlp.pipe_names,'\\n')\n",
    "print(nlp.pipeline,'\\n')\n",
    "print(nlp.pipe_factories,'\\n')\n",
    "print(nlp.pipe_labels,'\\n')\n",
    "print(nlp.pipe,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc     |    ORG\n",
      "$44 billion     |    MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc has acquired twitter for $44 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent, \"    |   \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " has acquired twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $44 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can also use another pre-trained pipeline, in another language**\n",
    "\n",
    "\n",
    "- Run python -m spacy download fr_core_news_sm to download pre-trained pipeline package is executing for 1st time\n",
    "- fr_core_news_sm is French language pre-trained pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "Twitter  |  MISC  |  Miscellaneous entities, e.g. events, nationalities, products or works of art\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(\"Tesla Inc va racheter Twitter pour $45 milliards de dollars\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla  |  PROPN  |  Tesla\n",
      "Inc  |  PROPN  |  Inc\n",
      "va  |  VERB  |  aller\n",
      "racheter  |  VERB  |  racheter\n",
      "Twitter  |  VERB  |  twitter\n",
      "pour  |  ADP  |  pour\n",
      "$  |  NOUN  |  dollar\n",
      "45  |  NUM  |  45\n",
      "milliards  |  NOUN  |  milliard\n",
      "de  |  ADP  |  de\n",
      "dollars  |  NOUN  |  dollar\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \" | \", token.pos_, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a Component to a blank pipeline\n",
    "\n",
    "- Take a pre trained pipeline and create a blank pipeline for it\n",
    "- Copy the compomentd from pre-trained pipeline to blank pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "cust_nlp = spacy.blank(\"en\")\n",
    "\n",
    "cust_nlp.add_pipe(\"ner\", source=nlp)\n",
    "cust_nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc ORG\n",
      "$45 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
